---
title: "P8105 Final Project Report"
author: "By: Tanya Butt, Catherine Lucey, and Irene Martinez Morata"
date: "12/11/2021"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(ggplot2)
library(lubridate)
library(ggridges)
library(purrr)
library(plotly)
library(patchwork)


getwd()
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",

  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Covid-19 And Specialized High School Admissons in New York City

Exploring the Potential Associations between Specialized High School Admissions Offers and COVID-19 Rates in NYC

## Motivation and Related Work

## Project Aims
1) Identify and evaluate the Covid-19 positive rate incidence, death rate, high school applications and SH SAT testers and offers in New York City during 2020 and the previous 4 years.
2) Compare the time-trends of SH SAT test takers and from 2015 to 2020
3) Visualize the geographical distribution of Covid-19 incidence, death rate and proportion of SG SAT offers during 2020 across MOZCTA
4) Assess the association between Covid-19 positive rate cumulative incidence and the number of students that got an SH SAT offer in 2020 by MOZCTA in New York City.
5) Assess the association between Covid-19 death rate by 100,000 inhabitants and the number of students that got an SH SAT offer in 2020 by MOZCTA in New York City.

## Data Cleaning

### SHSAT Testing and Offers Dataset

##### Loading and Cleaning Yearly SHSAT Data

Due to small differences in variable naming between 2015-2017 and 2018-2020, data importing and cleaning was done in two parts using the functions `clean_shsatv1` and `clean_shsatv2`. In the original data sets, the number of offers for schools where five or fewer students received an offer to a specialized high school was recorded as "0-5," thus we could not distinguish between schools that had no students receiving a specialized high school offer and schools that had up to five students receiving an offer. 0-5 values for `n_offers` were converted to "5," and all values in the column were converted to numerics for easy manipulation. School years, of the format 2015-2016, were re-coded as the year of the fall semester: `year = 2015` represents school year that began in Fall 2015 and ended in Spring 2016, in which eighth graders took the SHSAT in the fall of 2015 and received high school offers in the spring of 2016. Lastly, duplicates were removed from the combined SHSAT dataset with the `unique()` function, removing 565 values. We suspect that all duplicates are from the 2019 dataset, which seems to mysteriously duplicate in the `rbind()` process.

```{r}

# define functions to load and clean each .csv

clean_shsatv1 = function(df){
  clean_df = 
    df %>% 
    janitor::clean_names() %>% 
    rename(
      dbn = feeder_school_dbn,
      feeder_school = feeder_school_name,
      n_offers = count_of_offers,
      n_testers = count_of_testers,
      n_hs_applicants = count_of_students_in_hs_admissions,
      ) %>% 
    mutate(
      n_hs_applicants = 
        replace(n_hs_applicants, n_hs_applicants == "0-5", "5"),
      n_hs_applicants = as.double(n_hs_applicants)
    ) %>% 
    mutate(
      n_offers = 
        replace(n_offers, n_offers == "0-5", "5"),
      n_offers = as.double(n_offers)
    ) %>% 
    mutate(
      n_testers =
        replace(n_testers, n_testers == "0-5", "5"),
      n_testers = as.double(n_testers)
    )
  
  return(clean_df)
}

clean_shsatv2 = function(df){
  clean_df =
    df %>% 
    janitor::clean_names() %>% 
    rename(
      dbn = feeder_school_dbn,
      feeder_school = feeder_school_name,
      n_offers = number_of_offers,
      n_testers = count_of_testers,
      n_hs_applicants = count_of_students_in_hs_admissions,
      ) %>% 
    mutate(
      n_hs_applicants = 
        replace(n_hs_applicants, n_hs_applicants == "0-5", "5"),
      n_hs_applicants = as.double(n_hs_applicants)
    ) %>% 
    mutate(
      n_offers = 
        replace(n_offers, n_offers == "0-5", "5"),
      n_offers = as.double(n_offers)
    ) %>% 
    mutate(
      n_testers =
        replace(n_testers, n_testers == "0-5", "5"),
      n_testers = as.double(n_testers)
    )
  
  return(clean_df)
}

```

```{r load_clean_shsat_data, message=FALSE, warning=FALSE}

# import and clean each of the six years of SHSAT data individually

fifteen_df = 
  read_csv("raw_data/shsat_1/2015-2016_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2015") %>% 
  clean_shsatv1()

sixteen_df =
  read_csv("raw_data/shsat_1/2016-2017_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2016") %>% 
  clean_shsatv1()

seventeen_df =
  read_csv("raw_data/shsat_1/2017-2018_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2017") %>% 
  clean_shsatv1()

eighteen_df =
  read_csv("raw_data/shsat_1/2018-2019_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2018") %>% 
  clean_shsatv2()

nineteen_df =
  read_csv("raw_data/shsat_1/2019-2020_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2019") %>% 
  clean_shsatv2()

twenty_df =
  read_csv("raw_data/shsat_1/2020-2021_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2020") %>% 
  clean_shsatv2()

# bind all years into a cohesive dataframe, filter for unique values

shsat_df = 
  rbind(
    fifteen_df, 
    sixteen_df, 
    seventeen_df, 
    eighteen_df, 
    nineteen_df, 
    twenty_df) %>% 
  unique()

```

These data were then joined with the DOE 2020 Middle School Directory dataset to obtain a zip code for each sending middle school, and exported for use in subsequent analyses.

```{r import_ms_data, message=FALSE}
ms_dir_import =
  read_csv("raw_data/2019_DOE_Middle_School_Directory.csv") %>% 
  janitor::clean_names() %>% 
  select(schooldbn, borough, postcode, address) %>% 
  rename(
    dbn = schooldbn,
    zip = postcode
  )

shsat_zip_df =
  full_join(shsat_df, ms_dir_import, by = "dbn") %>% 
  select(dbn, n_hs_applicants, n_testers, n_offers, year, borough, zip) %>% 
  drop_na()

# write.csv(shsat_zip_df, "shsat_data.csv")

```

Because the DOE middle school directory only includes data for public middle schools, addresses were not available for charter schools and thus charter schools were excluded from this analysis (max of n = 191 in 2020), leaving data for 464 schools for 2015-2017, 465 schools for 2018 and 2019, and 467 schools in 2020 (See table below).

```{r n_excluded}

# keep a version of the shsat_zip_df that retains NA values just to count NAs

full_shsat_zip_df =
  full_join(shsat_df, ms_dir_import, by = "dbn")

n_kept =
  full_shsat_zip_df %>%
  select(zip, year) %>% 
  filter(!is.na(zip)) %>% 
  group_by(year) %>%
  count()

n_na =
  full_shsat_zip_df %>%
  select(zip, year) %>% 
  filter(is.na(zip)) %>% 
  group_by(year) %>%
  count()

n_removed_table =
  full_join(n_na, n_kept, by = "year") %>% 
  mutate(
    `Number of NA Values Removed` = n.x,
    `Number of Schools Kept` = n.y
  ) %>% 
  select(year,`Number of NA Values Removed`, `Number of Schools Kept` ) %>% 
  knitr::kable() %>% 
  print()

```

Filtering out schools with fewer than 5 offers in any given year (`n_offers < 5`) left approximately 120 schools per year. This subset was used in all subsequent analysis.

```{r message=FALSE}

  shsat_zip_df %>%
  filter(n_offers > 5) %>%
  group_by(year) %>% 
  summarise(n_schools = n()) %>% 
  knitr::kable() %>% 
  print()

```


Lastly, we calculated the weighted average of offers for all years, 2015-2019, and 2020 by using the `weighted.mean()` function, weighting the number of offers at a school by the number of students applying to specialized high schools, and averaging across all schools within a ZCTA. All weighted averages were calculated after filtering to select only schools with > 5 offers total. Furthermore, `n_offers` and `n_testers` were similarly averaged within a ZCTA and for all years, 2015-2019, and 2020 alone.

```{r wt_avgs_df, message=FALSE}

filtered_wt_avgs_df =
  read_csv("raw_data/project_df.csv") %>%
  select(modzcta, dbn, n_testers, n_offers, year) %>%
  drop_na() %>% 
  filter(n_offers > 5) %>% 
  group_by(modzcta, year) %>% 
  summarize(
    avg_testers = mean(n_testers),
    avg_offers = mean(n_offers),
    wt_avg = weighted.mean(n_offers, n_testers)
  ) %>% 
  pivot_wider(names_from = year, values_from = avg_testers:wt_avg) %>%
  drop_na() %>% 
  mutate(
    `2015-19_avg_testers` = mean(avg_testers_2015:avg_testers_2019),
    `2015-19_avg_offers` = mean(avg_offers_2015:avg_testers_2019),
    `2015-19_wt_avg` = mean(wt_avg_2015:wt_avg_2019),
    all_years_avg_testers = mean(avg_testers_2015:avg_testers_2020),
    all_years_avg_offers = mean(avg_offers_2015:avg_testers_2020),
    all_years_wt_avg = mean(wt_avg_2015:wt_avg_2020)
  ) %>% 
  select(
    all_years_avg_testers, 
    all_years_avg_offers, 
    all_years_wt_avg, 
    `2015-19_avg_testers`,
    `2015-19_avg_offers`, 
    `2015-19_wt_avg`, 
    avg_testers_2020, 
    avg_offers_2020, 
    wt_avg_2020
    )

# write.csv(wt_avgs_df, "filtered_weighted_avgs.csv")

```

### Covid-19 cumulative incidence rate in 2020

The Covid-19 cumulative incidence rate in 2020 was imported directly from the github repository of the [NYC Department of Health and Mental Hygiene](https://github.com/nychealth/coronavirus-data)
This data set contains the cumulative Covid-19 positive incidence rate from March 2020 until December 2020. In this data set, a person is classified as a confirmed COVID-19 case if they test positive with a molecular test.This data set include people who live in NYC. Any person with a residence outside of NYC is not included. Children attending school and their families should be residents in NYC to do so, thus, this is a potential benefit of using this data set, which is excluding the "noise" from external individuals testing positive for Covid-19 in a neighborhood but not residing there.


```{r covid positive data, message=FALSE}
test2 <- read_csv("raw_data/tests-by-zcta.csv")
```

### MODZCTA level shapefile

To obtain consistent area units  across the datasets, we used a [crosswalk file](https://github.com/nychealth/coronavirus-data/blob/master/Geography-resources/ZCTA-to-MODZCTA.csv) to convert zip-codes to MODZCTAs. MODZCTAs, based on the 2010 U.S. Census, arecurrently used by the NYC DOHMH for the mapping of COVID-19. The use of MODZCTA as out spatial level allow us to overcome several limitations of the zip code level data (i.e., A ZIP Code doesnâ€™t actually refer to an area, but rather a collection of points that make up a mail delivery route). The modified ZCTA (MODZCTA) geography combines census blocks with smaller populations to allow more stable estimates of population size for rate calculation. Information by geography reflect people's MODZCTA of residence at the time of reporting, and not the location of testing, diagnosing, or hospitalization. The ZCTA geography was developed by the U.S. Census Bureau.


### Variable Dictionary

`n_offers` = total number of eighth grade students who received an offer to a specialized high school at a given middle school

`n_testers` = total number of eighth grade students taking the SHSAT exam, equivalent to the number of students applying to specialized high schools at a given middle school

`n_hs_applicants` = total number of eighth grade students participating in the high school applications process (applicants to specialized + non-specialized high schools) at a given middle school

`dbn` = district borough number, a unique school identification number assigned by the NYC DOE

`feeder_school` = full name of the "sending" middle school (middle school at which applicants are currently enrolled)

`modzcta_cum_perc_pos` = cumulative incidence of confirmed Covid-19 positive cases from March 2020 until December 2020 in NYC.


## Data Join
In order to join all the separate data sets, some of which were at the zip code level, we used an additional zip code-modzcta conversion file to convert them to such levels. 
Once all datasets had a modzcta variable, we joined them together.

```{r joining_chunk, message=FALSE}
covid_test <- read_csv("raw_data/tests-by-zcta.csv")
covid_deathrate <- read_csv("raw_data/deathrate_byzcta_clean.csv")
school_data_clean <- read_csv("raw_data/filtered_weighted_avgs.csv")

zip_mod <- read_csv("raw_data/ZCTA-to-MODZCTA.csv") %>% 
  rename(zip = ZCTA)

school <- read_csv("raw_data/shsat_data.csv") 

covid_deathrate <- read_csv("raw_data/deathrate_byzcta_clean.csv")

zcta_skool <- full_join(zip_mod, school, by = "zip") %>% 
  rename(
     modzcta = MODZCTA
  ) %>% 
  select(-`...1`)

zcta_skool_covidtest = full_join(zcta_skool, covid_test, by = "modzcta") %>% 
  janitor::clean_names()

project_df_deathrate = 
  full_join(zcta_skool_covidtest, covid_deathrate, by = "modzcta") %>% 
  janitor::clean_names()


# write_csv(zcta_skool_covidtest, "project_df.csv")

```

```{r joining_chunk2, message=FALSE}

school_covidtest <- full_join(school_data_clean, covid_test, by = "modzcta") %>% 
  janitor::clean_names()

final_mod_zcta <- full_join(school_covidtest, covid_deathrate, by = "modzcta") %>% 
  janitor::clean_names() %>% 
  rename_with(~ gsub('x', '', .x)) 


# write_csv(final_mod_zcta, "final_mod_zcta.csv")

```

# Results

## Exploratory Data Analysis (EDA)

#### The Distribution of Specialized High School Offers in New York City

*Figure 1: Proportion of Offers by Number of Testers Across NYC Boroughs, 2015-2020* 

```{r hs_admits_overview, message=FALSE, fig.width=10, fig.height=8}

shsat_covid_df = 
  read_csv("raw_data/project_df.csv") %>%
  mutate(
    year = factor(year, levels = c("2015", "2016", "2017", "2018", "2019", "2020"))
  ) %>% 
  select(zip, modzcta, borough, 
         dbn, year, n_hs_applicants, n_testers, n_offers, 
         modzcta_cum_perc_pos, cum_ave_death_rate
         )

shsat_covid_df %>%
  filter(n_offers > 5) %>% 
  filter(year != 2015) %>% 
  mutate(
    offers_per_applicants = n_offers / n_hs_applicants
  ) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = offers_per_applicants)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.1, color = "red") +
  facet_grid(. ~borough) +
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "bottom") +
  labs(
    title = "Borough and Year Differences in Number of Specialized High School Admissions Offers",
    subtitle = "Offers per applicants is the fraction of students among those applying to specialized schools who receive offers",
    x = "Year",
    y = "Offers Per Applicants"
  )
```

*Figure 2: Number of Offers by Proportion of Students Additionally Applying to Specialized Schools* 

```{r teters_vs_allapps, message=FALSE, warning=FALSE, fig.width=10, fig.height=9}

shsat_covid_df %>%
  filter(n_offers > 5,
         year != 2015) %>% 
  mutate(
    spec_per_applicants = n_testers / n_hs_applicants,
    offers_per_tester = n_offers / n_testers
  ) %>% 
  group_by(year) %>% 
  ggplot(aes(y = offers_per_tester, x = spec_per_applicants)) +
  geom_point() +
  geom_smooth(method = lm) +
  facet_grid(. ~borough) +
  labs(
    title = "Do schools with more students applying to specialized schools get more offers?",
    x = "Proportion of HS Applicants Applying to Specialized High Schools",
    y = "Proportion of Offers to # Specailized HS Applicants"
  ) +
  theme(axis.text.x = element_text(angle = 90))

```

Since applying to most high schools and to specialized high schools are two separate processes, we were interested in whether there is a relationship between how common it is to apply to specialized high schools at a given middle school, and how many specialized high school admissions offers that school receives. The X axis of Figure 2 describes: out of the students applying to high school, how many are also applying to specialized high schools? The Y axis then adds: how many students receive offers to specialized high schools for different prevalence of specialized high school applications?

Note that data from 2015 are excluded from this figure as the number of students applying specifically to specialized high school applicants was the same as the number of students in general high school admissions, suggesting that the number of students in general admissions was not recorded for 2015.

In the Bronx and on Staten Island, there are few schools where a majority of the students applying to high school are additionally applying to specialized high schools, and of the students who do apply to specialized high schools, few receive an offer. In Brooklyn, Manhattan, and Queens, the greatest proportion of admissions offers per specialized high school applicant are received at schools where most students apply to specialized high schools. In Brooklyn and Queens, there are some schools where most students apply to specialized high schools, but few get in. Overall, these data suggest that the middle school a student attends is strongly correlated with how likely that student is to be offered admission to a specialized high school.

#### Specialized High School Offers and Covid-19

*Figure 3: Number of Offers Before and During the Covid-19 Pandemic*

```{r wt_avg_covid_precovid, message=FALSE, warning=FALSE, fig.width=9.5,fig.height=14}

# Re-define a df with weighed averages to include borough (which is excluded in the final_mod_zcta file)

zcta_wt_avgs_df =
  read_csv("raw_data/project_df.csv") %>%
  select(modzcta, dbn, borough, n_hs_applicants, n_testers, n_offers, year, modzcta_cum_perc_pos, cum_ave_death_rate) %>%
  filter(n_offers > 5) %>% 
  drop_na() %>% 
  mutate(
    offers_per_testers = n_offers / n_testers,
    spec_per_applicants = n_testers / n_hs_applicants
  ) %>% 
  group_by(modzcta, year, borough) %>% 
  summarize(
    avg_testers = mean(n_testers),
    avg_offers = mean(n_offers),
    avg_applicants = mean(n_hs_applicants),
    wt_avg_offers = weighted.mean(n_offers, n_testers),
    avg_offer_per_tester = mean(offers_per_testers),
    avg_spec_per_applicants = mean(spec_per_applicants)
  )

# Plot the zipcode level weighted average number of offers by borough before and during the Covid-19 pandemic

precovid_boro_wt_avg =
  zcta_wt_avgs_df %>% 
  select(modzcta, year, wt_avg_offers, borough) %>%
  drop_na() %>% 
  group_by(modzcta, year, borough) %>%
  summarize(
    avg_wt_avg = mean(wt_avg_offers)
  ) %>% 
  pivot_wider(names_from = year, values_from = avg_wt_avg) %>%
  drop_na() %>% 
  mutate(
    precovid_wt_avg = mean(`2015`:`2019`)
    ) %>%
  ggplot(aes(x = borough, y = precovid_wt_avg)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.2, color = "red") +
  ylim(-10, 225) +
  labs(
    y = "Weighted Avg. # Offers", 
    x = "Borough",
    title = "Pre-Covid (2015-2019)"
    ) +
   theme(plot.title = element_text(hjust = 0.5))


covid_boro_wt_avg =
  zcta_wt_avgs_df %>% 
  filter(year == 2020) %>% 
  group_by(borough) %>% 
  ggplot(aes(x = borough, y = wt_avg_offers)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.2, color = "red") +
  ylim(-10, 225) +
  labs(
    y = "Weighted Avg. # Offers", 
    x = "Borough", 
    title = "Covid (2020)"
    ) +
  theme(plot.title = element_text(hjust = 0.5))

patchwork = precovid_boro_wt_avg + covid_boro_wt_avg

patchwork + plot_annotation(
  title = "Difference in Admissions Offers Before and During the Covid-19 Pandemic",
  subtitle = "Number of offers received at a school is weighted by the number of SHSAT test takers at that school, then averaged across ZCTA",
  theme(text = element_text(hjust = 0.5))
)
```

First, we wanted to see if there was a year-to-year difference in numbers of offers in each borough before 2020 and during 2020, to broadly capture the potential effects of the Covid-19 pandemic. To do so, we plotted the weighted average number of offers in 2020 and averaged from 2015-2019. The weighted average number of offers is the number of offers per school, weighted by the number of applicants at that school, averaged within a ZCTA. Pre-covid data (2015-2019) is the weighted average number of offers per ZCTA, averaged again across the years 2015-2019.

The weighted average number of offers seems to have stayed largely the same pre- and during the Covid-19 pandemic in the Bronx and on Staten Island, though Staten Island does seem to have a ZCTA where middle schools performed better in 2020 than in previous years. Interestingly, the median weighted average number of offers for zip codes Manhattan decreased in the pandemic, though there were still a few zip codes performing as well as in previous years. In Brooklyn, there are a few ZCTAs in which students had an unusually high number of offers in 2020, though, like Manhattan, the median number-of-applicant-weighted average number of offers per Brooklyn zip code is lower in 2020 than in the preceding five years.

*Figure 4: Covid Data and Offers of Admission* 

```{r covid_admit_rate_gg, message=FALSE, warning=FALSE, fig.width=10, fig.height=9}

# join in borough data that was excluded from the final analysis df
modzcta_to_borough = 
  shsat_covid_df %>% 
  select(modzcta, borough) %>% 
  unique()

final_shsat_modzcta =
  read_csv("raw_data/final_mod_zcta.csv") %>% 
  select(modzcta,
         all_years_avg_testers, 
         all_years_avg_offers, 
         all_years_wt_avg, 
         `2015_19_avg_testers`,	
         `2015_19_avg_offers`, 
         `2015_19_wt_avg`, 
         avg_testers_2020, 
         avg_offers_2020, 
         wt_avg_2020,	
         modzcta_cum_perc_pos,
         cum_ave_death_rate) %>% 
  drop_na() %>% 
  full_join(modzcta_to_borough, .x, by = "modzcta")
  
# plot weighted average n offers before and during Covid

wt_avg_vs_covidtest =
  final_shsat_modzcta %>% 
  ggplot(aes(x = modzcta_cum_perc_pos, y = wt_avg_2020)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(
   x = "Cumulative Covid-19 % Positivity 
   Rate 3/20 - 12/20",
   y = "Weighted Average # Offers Per ZCTA",
   title = "Positivity Rate"
  )

wt_avg_vs_death =
  final_shsat_modzcta %>%
  ggplot(aes(x = cum_ave_death_rate, y = wt_avg_2020)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(
    x = "Cumulative Avg Covid-19 Death Rate 
    Per 100,000 3/20 - 12/20",
    y = "Weighted Average # Offers Per ZCTA",
    title = "Death Rate"
  )

patchwork = wt_avg_vs_covidtest + wt_avg_vs_death

patchwork + plot_annotation(
  title = "Relationship Bewteen Covid Positvity Rate, Death Rate and Number of Offers per ZCTA"
)

```

We next wanted to understand whether the COVID-19 pandemic affected success in specialized high school admissions for 2020, and plotted the weighted average number of offers in a ZCTA against two measures of Covid rates, the cumulative percent positivity rate and the cumulative average Covid-19 death rate per 100,000 from March, 2020, to December, 2020.

There appears to be a weak negative linear association between the average COVID-19 percent positivity rate testing and the average number of offers for middle schools within a particular ZCTA. This suggests that zip codes that had high COVID-19 case rates also tend to have middle schools where few students receive admissions offers to specialized high schools. There appears to be no relationship between the average number of offers and the cumulative average COVID-19 death rate per 100,000. Thus our prediction for our linear models is that there will be an association with the COVID test positive rate, but no association with the average death rate.

## Primary Analysis

We used descriptive statistics to summarize the distribution of our dependent variable: the average number of SHSAT offers in 2020 by MOZCTA, and relevant predictors: the cumulative Covid-19 positive incidence rate and the cumulative death rate from March until December 2020.

```{r message=FALSE, error=FALSE}

data = read_csv("raw_data/final_mod_zcta.csv")
data = data %>% 
  filter(!is.na(modzcta_cum_perc_pos))
summary(data$modzcta_cum_perc_pos)
summary(data$cum_ave_death_rate)


```


We ran chi square tests to test if there was a significant difference between the number of offers in 2020 and the number of offers from 2015 until 2019. Additionally we tested if there was a significant difference between the number of testers in 2020 and the weighted average for the number of offers from 2015 until 2019.

```{r chisq, message=FALSE}

chisq_df =
  read_csv("raw_data/project_df.csv") %>%
  filter(n_offers > 5) %>% 
  select(modzcta, dbn, n_testers, n_offers, year) %>%
  drop_na() %>% 
  group_by(dbn, year) %>% 
  pivot_wider(names_from = year, values_from = n_testers:n_offers) %>%
  drop_na() %>% 
  summarize(
    `2015-19_avg_testers` = mean(n_testers_2015:n_testers_2019),
    `2020_testers` = n_testers_2020,
    `2015-19_avg_offers` = mean(n_offers_2015:n_offers_2019),
    `2020_offers` = n_offers_2020
  ) %>% 
  mutate(
    `2015-19_prop` = `2015-19_avg_offers` / `2015-19_avg_testers`,
    `2020_prop` = `2020_offers` / `2020_testers`
  )

chi1 = chisq_df %>% 
  select(`2015-19_avg_testers`, `2020_testers`) %>% 
  chisq.test() 

chi1_df =
  chi1 %>% 
  broom::tidy() %>% 
  dplyr::select(statistic, p.value) %>% 
  mutate(statistic = "Number of Testers")

knitr::kable(chi1_df, digits = 5)

chi2 = chisq_df %>% 
  select(`2015-19_avg_offers`, `2020_offers`) %>% 
  chisq.test() 

chi2_df =
  chi2 %>% 
  broom::tidy() %>% 
  dplyr::select(statistic, p.value) %>% 
  mutate(statistic = "Number of Offers")

knitr::kable(chi2_df, digits = 5)

```

## Additional Analysis: Modeling
For our secondary analysis, we excluded all schools with less than 5 students receiving offers to specialized high schools to avoid positivity violations. 
Because our dependent variable is a count (number of offers), we used a poisson linear regression. However, because the distribution of the data was overspread, the variance did not equal the mean. To avoid the violation of this assumption, we used a  Quasi-Poisson model, which assumes that the variance is a linear function of the mean. In our primary model, we calculated the change in the number of SHSAT offers by a 10% increase in the cumulative Covid-19 incidence. We progressively adjusted the model, including the total number of testers by MOZCTA. Additionally, we run a secondary model with 10% increase in the cumulative mortality rate as the main predictor.

```{r message=FALSE, warning=FALSE}
#this is the main model (10% change in cumulative incidence as predictor)

data <- read_csv("raw_data/final_mod_zcta.csv") %>% 
  mutate(all_years_avg_offers = round(all_years_avg_offers, digits = 0), 
         all_years_wt_avg_offers = round(all_years_avg_offers, digits = 0),
         wt_avg_2020 = round(wt_avg_2020, digits = 0)) %>% 
  mutate(cum_perc = 10*(modzcta_cum_perc_pos),
         cum_death = 10*(cum_ave_death_rate))


model1 = glm(avg_offers_2020 ~ cum_perc , family = quasipoisson, data = data)

model1 %>% 
  broom::tidy() %>% 
  dplyr::select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "cum_perc", "Cumulative incidence ")) %>% 
  knitr::kable(digits = 3)

#model adjusted by number of testers
model3 = glm(avg_offers_2020 ~ cum_perc + avg_testers_2020, family = quasipoisson, data = data)

model3 %>% 
  broom::tidy() %>% 
  dplyr::select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "cum_perc", "Cumulative incidence ")) %>% 
  knitr::kable(digits = 3)

#model adjusted by number of testers and death rate
model4 = glm(avg_offers_2020 ~ cum_perc + avg_testers_2020 + cum_death, family = quasipoisson, data = data)

model4 %>% 
  broom::tidy() %>% 
  dplyr::select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "cum_perc", "Cumulative incidence ")) %>% 
  knitr::kable(digits = 3)

#model with a 10% change in death rate as the main predictor
model2 = glm(wt_avg_2020 ~ cum_death, family = quasipoisson, data = data)

model2 %>% 
  broom::tidy() %>% 
  dplyr::select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "cum_death", "Cumulative death rate")) %>% 
  knitr::kable(digits = 5)

```

## Discussion

 