---
title: "P8105 Final Project Report"
author: "By: Tanya Butt, Catherine Lucey, and Irene Martinez Morata"
date: "12/11/2021"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(ggplot2)
library(lubridate)
library(ggridges)
library(purrr)
library(plotly)


getwd()
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",

  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Covid-19 And Specialized High School Admissons in New York City

Exploring the Potential Associations between Specialized High School Admissions Offers and COVID-19 Rates in NYC

## Motivation and Related Work

## Project Aims
1) Identify and evaluate the Covid-19 positive rate incidence, death rate, high school applications and SH SAT testers and offers in New York City during 2020 and the previous 4 years.
2) Compare the time-trends of SH SAT test takers and from 2016 to 2020
3) Visualize the geographical distribution of Covid-19 incidence, death rate and proportion of SG SAT offers during 2020 across MOZCTA
4) Assess the association between Covid-19 positive rate cumulative incidence and the number of students that got an SH SAT offer in 2020 by MOZCTA in New York City.
5) Assess the association between Covid-19 death rate by 100,000 inhabitants and the number of students that got an SH SAT offer in 2020 by MOZCTA in New York City.

## Data Cleaning

### SHSAT Testing and Offers Dataset

##### Loading and Cleaning Yearly SHSAT Data

Due to small differences in variable naming between 2015-2017 and 2018-2020, data importing and cleaning was done in two parts using the functions `clean_shsatv1` and `clean_shsatv2`. In the original data sets, the number of offers for schools where five or fewer students received an offer to a specialized high school was recorded as "0-5," thus we could not distinguish between schools that had no students receiving a specialized high school offer and schools that had up to five students receiving an offer. 0-5 values for `n_offers` were converted to "5," and all values in the column were converted to numerics for easy manipulation. School years, of the format 2015-2016, were re-coded as the year of the fall semester: `year = 2015` represents school year that began in Fall 2015 and ended in Spring 2016, in which eighth graders took the SHSAT in the fall of 2015 and received high school offers in the spring of 2016. Lastly, duplicates were removed from the combined SHSAT dataset with the `unique()` function, removing 565 values. We suspect that all duplicates are from the 2019 dataset, which seems to mysteriously duplicate in the `rbind()` process.

```{r}

# define functions to load and clean each .csv

clean_shsatv1 = function(df){
  clean_df = 
    df %>% 
    janitor::clean_names() %>% 
    rename(
      dbn = feeder_school_dbn,
      feeder_school = feeder_school_name,
      n_offers = count_of_offers,
      n_testers = count_of_testers,
      n_hs_applicants = count_of_students_in_hs_admissions,
      ) %>% 
    mutate(
      n_hs_applicants = 
        replace(n_hs_applicants, n_hs_applicants == "0-5", "5"),
      n_hs_applicants = as.double(n_hs_applicants)
    ) %>% 
    mutate(
      n_offers = 
        replace(n_offers, n_offers == "0-5", "5"),
      n_offers = as.double(n_offers)
    ) %>% 
    mutate(
      n_testers =
        replace(n_testers, n_testers == "0-5", "5"),
      n_testers = as.double(n_testers)
    )
  
  return(clean_df)
}

clean_shsatv2 = function(df){
  clean_df =
    df %>% 
    janitor::clean_names() %>% 
    rename(
      dbn = feeder_school_dbn,
      feeder_school = feeder_school_name,
      n_offers = number_of_offers,
      n_testers = count_of_testers,
      n_hs_applicants = count_of_students_in_hs_admissions,
      ) %>% 
    mutate(
      n_hs_applicants = 
        replace(n_hs_applicants, n_hs_applicants == "0-5", "5"),
      n_hs_applicants = as.double(n_hs_applicants)
    ) %>% 
    mutate(
      n_offers = 
        replace(n_offers, n_offers == "0-5", "5"),
      n_offers = as.double(n_offers)
    ) %>% 
    mutate(
      n_testers =
        replace(n_testers, n_testers == "0-5", "5"),
      n_testers = as.double(n_testers)
    )
  
  return(clean_df)
}

```

```{r load_clean_shsat_data, message=FALSE, warning=FALSE}

# import and clean each of the six years of SHSAT data individually

fifteen_df = 
  read_csv("raw_data/shsat_1/2015-2016_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2015") %>% 
  clean_shsatv1()

sixteen_df =
  read_csv("raw_data/shsat_1/2016-2017_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2016") %>% 
  clean_shsatv1()

seventeen_df =
  read_csv("raw_data/shsat_1/2017-2018_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2017") %>% 
  clean_shsatv1()

eighteen_df =
  read_csv("raw_data/shsat_1/2018-2019_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2018") %>% 
  clean_shsatv2()

nineteen_df =
  read_csv("raw_data/shsat_1/2019-2020_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2019") %>% 
  clean_shsatv2()

twenty_df =
  read_csv("raw_data/shsat_1/2020-2021_SHSAT_Admissions_Test_Offers_By_Sending_School.csv") %>% 
  mutate(year = "2020") %>% 
  clean_shsatv2()

# bind all years into a cohesive dataframe, filter for unique values

shsat_df = 
  rbind(
    fifteen_df, 
    sixteen_df, 
    seventeen_df, 
    eighteen_df, 
    nineteen_df, 
    twenty_df) %>% 
  unique()

```

These data were then joined with the DOE 2020 Middle School Directory dataset to obtain a zip code for each sending middle school, and exported for use in subsequent analyses.

```{r import_ms_data, message=FALSE}
ms_dir_import =
  read_csv("raw_data/2019_DOE_Middle_School_Directory.csv") %>% 
  janitor::clean_names() %>% 
  select(schooldbn, borough, postcode, address) %>% 
  rename(
    dbn = schooldbn,
    zip = postcode
  )

shsat_zip_df =
  full_join(shsat_df, ms_dir_import, by = "dbn") %>% 
  select(dbn, n_hs_applicants, n_testers, n_offers, year, borough, zip) %>% 
  drop_na()

# write.csv(shsat_zip_df, "shsat_data.csv")

```

Because the DOE middle school directory only includes data for public middle schools, addresses were not available for charter schools and thus charter schools were excluded from this analysis (max of n = 191 in 2020), leaving data for 464 schools for 2015-2017, 465 schools for 2018 and 2019, and 467 schools in 2020 (See TABLE WHAT NUMBER???).

```{r n_excluded}

# keep a version of the shsat_zip_df that retains NA values just to count NAs

full_shsat_zip_df =
  full_join(shsat_df, ms_dir_import, by = "dbn")

n_kept =
  full_shsat_zip_df %>%
  select(zip, year) %>% 
  filter(!is.na(zip)) %>% 
  group_by(year) %>%
  count()

n_na =
  full_shsat_zip_df %>%
  select(zip, year) %>% 
  filter(is.na(zip)) %>% 
  group_by(year) %>%
  count()

n_removed_table =
  full_join(n_na, n_kept, by = "year") %>% 
  mutate(
    `Number of NA Values Removed` = n.x,
    `Number of Schools Kept` = n.y
  ) %>% 
  select(year,`Number of NA Values Removed`, `Number of Schools Kept` ) %>% 
  knitr::kable() %>% 
  print()

```

AT THIS POINT I'LL PUT IN DOING THE WEIGHTED AVERAGES ETC ETC ETC ETC

### Variable Dictionary

`n_offers` = total number of eighth grade students who received an offer to a specialized high school at a given middle school

`n_testers` = total number of eighth grade students taking the SHSAT exam, equivalent to the number of students applying to specialized high schools at a given middle school

`n_hs_applicants` = total number of eighth grade students participating in the high school applications process (applicants to specialized + non-specialized high schools) at a given middle school

`dbn` = district borough number, a unique school identification number assigned by the NYC DOE

`feeder_school` = full name of the "sending" middle school (middle school at which applicants are currently enrolled)


### Covid-19 cumulative incidence rate in 2020

The Covid-19 cumulative incidence rate in 2020 was imported directly from the github repository of the [NYC Department of Health and Mental Hygiene](https://github.com/nychealth/coronavirus-data)
This data set contains the cumulative Covid-19 positive incidence rate from March 2020 until December 2020. In this data set, a person is classified as a confirmed COVID-19 case if they test positive with a molecular test.This data set include people who live in NYC. Any person with a residence outside of NYC is not included. Children attending school and their families should be residents in NYC to do so, thus, this is a potential benefit of using this data set, which is excluding the "noise" from external individuals testing positive for Covid-19 in a neighborhood but not residing there.

### Variable Dictionary
`modzcta_cum_perc_pos` = cumulative incidence of confirmed Covid-19 positive cases from March 2020 until December 2020 in NYC.

```{r covid positive data}
test2 <- read.csv("tests-by-zcta.csv")
```

## MODZCTA level shapefile

To obtain consistent area units  across the datasets, we used a [crosswalk file](https://github.com/nychealth/coronavirus-data/blob/master/Geography-resources/ZCTA-to-MODZCTA.csv) to convert zip-codes to MODZCTAs. MODZCTAs, based on the 2010 U.S. Census, arecurrently used by the NYC DOHMH for the mapping of COVID-19. The use of MODZCTA as out spatial level allow us to overcome several limitations of the zip code level data (i.e., A ZIP Code doesnâ€™t actually refer to an area, but rather a collection of points that make up a mail delivery route). The modified ZCTA (MODZCTA) geography combines census blocks with smaller populations to allow more stable estimates of population size for rate calculation. Information by geography reflect people's MODZCTA of residence at the time of reporting, and not the location of testing, diagnosing, or hospitalization. The ZCTA geography was developed by the U.S. Census Bureau.




## Exploratory Data Analysis (EDA)
We used descriptive statistics to summarize the distribution of our dependent variable: the average number of SHSAT offers in 2020 by MOZCTA, and relevant predictors: the cumulative Covid-19 positive incidence rate and the cumulative death rate from March until December 2020.


## Additional Analysis: Modeling


For our secondary analysis, we excluded all schools with less than 5 students receiving offers to specialized high schools to avoid positivity violations. 
Because our dependent variable is a count (number of offers), we used a poisson linear regression. However, because the distribution of the data was overspread, the variance did not equal the mean. To avoid the violation of this assumption, we used a  Quasi-Poisson model, which assumes that the variance is a linear function of the mean. In our primary model, we calculated the change in the number of SHSAT offers by a 10% increase in the cumulative Covid-19 incidence. We progressively adjusted the model, including the total number of testers by MOZCTA. Additionally, we run a secondary model with 10% increase in the cumulative mortality rate as the main predictor.

## Discussion

 